{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImitationLearning-Intent-V2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3j__1QFRpDhC"
      },
      "source": [
        "# Primitive Segmentation using ConvLSTM\n",
        "\n",
        "**Author:** Aditya Jain <br>\n",
        "**Date started:** 27th July, 2020<br>\n",
        "**Description:** Predict primitves actions in a human demonstration using ConvLSTM model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZFJSlMAbpDhE"
      },
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7gn6YDneJAa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "10428cd2-2323-45b9-e39e-540ba0d201ce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1_bJF4iepDhF",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "import os\n",
        "from glob import glob"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeqMeiJCzxTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_CLASSES  = 2          # no. of primitves in the TADL\n",
        "PX         = 256        # no. of rows in training/test images\n",
        "PY         = 256        # no. of columns in training/test images\n",
        "CHANNELS   = 3          # no. of channels in the image\n",
        "N_FRAMES   = 15         # no. of frames in each training/test video\n",
        "BATCH_SIZE = 32         # size of the batches\n",
        "DATA_DIR   = \"/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkiq6-1Aejkz",
        "colab_type": "text"
      },
      "source": [
        "## Building the Dataset\n",
        "\n",
        "Reads the datafiles and builds the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBiC-UB6bPzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "00830c52-83d3-460d-c7d5-79db533d3dda"
      },
      "source": [
        "def video_path(dataset_dir):\n",
        "  '''\n",
        "  returns the paths of all video files in the dataset; takes input the parent directory\n",
        "  '''\n",
        "  # no. of primitives in the library\n",
        "  prim_actions    = [dI for dI in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir,dI))]\n",
        "  video_path_list = []\n",
        "  \n",
        "  # building a dictionary of primtive actions for label generation\n",
        "  prim_dict       = {}\n",
        "  i               = 0\n",
        "\n",
        "  for action in prim_actions:\n",
        "    prim_dict[action] = i\n",
        "    prim_path         = os.path.join(DATA_DIR, action)   # gives path for each primitive  \n",
        "    i                 += 1\n",
        "  \n",
        "    for video in os.listdir(prim_path):\n",
        "      video_path  = os.path.join(prim_path, video)        # path to all videos in a prim\n",
        "      video_path_list.append(video_path)\n",
        "      \n",
        "  return video_path_list, prim_dict\n",
        "\n",
        "\n",
        "video_list, primitives_dict = video_path(DATA_DIR)\n",
        "N_CLASSES                   = len(primitives_dict)\n",
        "\n",
        "\n",
        "def build_dataset(vid_list, primit_dict):\n",
        "    '''\n",
        "    This function builds the dataset given the video frames and no of primitives\n",
        "    '''\n",
        "\n",
        "    image_data  = []\n",
        "    label_data  = []\n",
        "\n",
        "    for video_path in vid_list:\n",
        "\n",
        "      label        = video_path.split(os.sep)[-2]\n",
        "      label        = primit_dict[label]\n",
        "      temp_image   = []\n",
        "      temp_label   = []\n",
        "    \n",
        "      for image in os.listdir(video_path):\n",
        "        image_path    = video_path + \"/\" + image \n",
        "      \n",
        "        # taking care of labels\n",
        "        # temp_label.append(label)\n",
        "\n",
        "        # load the raw data from the file as a string\n",
        "        img = tf.io.read_file(image_path)      \n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, [PX, PY])\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32) # Cast and normalize the image to [0,1]\n",
        "        temp_image.append(img)\n",
        "\n",
        "      image_data.append(temp_image)\n",
        "      temp_label = label       # temporary\n",
        "      label_data.append(temp_label)\n",
        "\n",
        "    return  image_data,  tf.one_hot(label_data, len(primit_dict))\n",
        "\n",
        "train_data, train_label = build_dataset(video_list, primitives_dict)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_data, train_label))\n",
        "\n",
        "## Trying to print the data\n",
        "# for itemx, itemy in dataset.take(3):\n",
        "#   print(itemx.shape, itemy.shape)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Grasp': 0, 'Approach': 1}\n",
            "tf.Tensor(\n",
            "[[1. 0.]\n",
            " [0. 1.]], shape=(2, 2), dtype=float32)\n",
            "(15, 256, 256, 3) (2,)\n",
            "(15, 256, 256, 3) (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvM7HYA7iu-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3676d9ac-d2fd-4eb8-8062-a231fbebbb48"
      },
      "source": [
        "test = \"/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Grasp/Video_1\"\n",
        "testt = test.split(os.sep)\n",
        "print(testt)\n",
        "print(testt[-2])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'content', 'drive', 'My Drive', 'TCS FullTime Work', 'LfD', 'Liquid_Pouring', 'TADL', 'Grasp', 'Video_1']\n",
            "Grasp\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8gcZ-1lkpDhK"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "We create a model which take as input movies of shape\n",
        "`(n_frames, width, height, channels)` and returns a movie\n",
        "of identical shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD19WMc5yPHz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e4780126-904e-4dc0-890a-cb4460d8a326"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape = (None, PX, PY, CHANNELS)),  \n",
        "        layers.ConvLSTM2D(\n",
        "            filters=64, kernel_size=(3, 3), padding=\"same\", return_sequences=False\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ConvLSTM2D(\n",
        "            filters=64, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ConvLSTM2D(\n",
        "            filters=64, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.5),\n",
        "        \n",
        "        # layers.Dense(100, activation='relu'),\n",
        "        layers.Dense(N_CLASSES, activation='softmax'),\n",
        "        \n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# model compilation\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9f38a356a19f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# layers.Dense(100, activation='relu'),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layers, name)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    219\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 926\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2644\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0mlast_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlast_dim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m       raise ValueError('The last dimension of the inputs to `Dense` '\n\u001b[0m\u001b[1;32m   1169\u001b[0m                        'should be defined. Found `None`.')\n\u001b[1;32m   1170\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlast_dim\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The last dimension of the inputs to `Dense` should be defined. Found `None`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hM9IeKmypDhV"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-bBV_l6mpDhW",
        "colab": {}
      },
      "source": [
        "epochs = 2\n",
        "\n",
        "model.fit(\n",
        "    image_data,\n",
        "    batch_size=10,\n",
        "    epochs=epochs,\n",
        "    verbose=2,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLnCLohPkYO9",
        "colab_type": "text"
      },
      "source": [
        "## Miscellaneous"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bf_VkgzwpDha"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "#### Test the model on one movie\n",
        "\n",
        "Feed it with the first 7 positions and then\n",
        "predict the new positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y2qaZa3rpDhb",
        "colab": {}
      },
      "source": [
        " movie_index = 1004\n",
        "track = noisy_movies[movie_index][:7, ::, ::, ::]\n",
        "\n",
        "for j in range(16):\n",
        "    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::])\n",
        "    new = new_pos[::, -1, ::, ::, ::]\n",
        "    track = np.concatenate((track, new), axis=0)\n",
        "\n",
        "\n",
        "# And then compare the predictions\n",
        "# to the ground truth\n",
        "track2 = noisy_movies[movie_index][::, ::, ::, ::]\n",
        "for i in range(15):\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "    ax = fig.add_subplot(121)\n",
        "\n",
        "    if i >= 7:\n",
        "        ax.text(1, 3, \"Predictions !\", fontsize=20, color=\"w\")\n",
        "    else:\n",
        "        ax.text(1, 3, \"Initial trajectory\", fontsize=20)\n",
        "\n",
        "    toplot = track[i, ::, ::, 0]\n",
        "\n",
        "    plt.imshow(toplot)\n",
        "    ax = fig.add_subplot(122)\n",
        "    plt.text(1, 3, \"Ground truth\", fontsize=20)\n",
        "\n",
        "    toplot = track2[i, ::, ::, 0]\n",
        "    if i >= 2:\n",
        "        toplot = shifted_movies[movie_index][i - 1, ::, ::, 0]\n",
        "\n",
        "    plt.imshow(toplot)\n",
        "    plt.savefig(\"%i_animate.png\" % (i + 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}