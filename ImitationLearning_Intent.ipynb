{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImitationLearning-Intent",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityajain07/LfD_ConvLSTM/blob/master/ImitationLearning_Intent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3j__1QFRpDhC"
      },
      "source": [
        "# Primitive Segmentation using ConvLSTM\n",
        "\n",
        "**Author:** Aditya Jain <br>\n",
        "**Date started:** 27th July, 2020<br>\n",
        "**Description:** Predict primitves actions in a human demonstration using ConvLSTM model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZFJSlMAbpDhE"
      },
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7gn6YDneJAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1_bJF4iepDhF",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "import os\n",
        "from glob import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeqMeiJCzxTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_CLASSES  = 2          # no. of primitves in the TADL\n",
        "PX         = 256        # no. of rows in training/test images\n",
        "PY         = 256        # no. of columns in training/test images\n",
        "CHANNELS   = 3          # no. of channels in the image\n",
        "N_FRAMES   = 15         # no. of frames in each training/test video\n",
        "BATCH_SIZE = 32         # size of the batches\n",
        "DATA_DIR   = \"/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkiq6-1Aejkz",
        "colab_type": "text"
      },
      "source": [
        "## Reading the Data\n",
        "\n",
        "Attempt 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBiC-UB6bPzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def video_path(dataset_dir):\n",
        "  '''\n",
        "  returns the paths of all video files in the dataset; takes input the parent directory\n",
        "  '''\n",
        "  # no. of primitives in the library\n",
        "  prim_actions    = [dI for dI in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir,dI))]\n",
        "\n",
        "  video_path_list = []\n",
        "\n",
        "  for action in prim_actions:\n",
        "    prim_path     = os.path.join(DATA_DIR, action)      # gives path for each primitive  \n",
        "  \n",
        "    for video in os.listdir(prim_path):\n",
        "      video_path  = os.path.join(prim_path, video)      # path to all videos in a prim\n",
        "      video_path_list.append(video_path)\n",
        "      \n",
        "  return video_path_list\n",
        "\n",
        "\n",
        "video_list = video_path(DATA_DIR)\n",
        "\n",
        "\n",
        "# this function builds the dataset\n",
        "def build_dataset(vid_list):\n",
        "\n",
        "    image_data  = []\n",
        "    label_data  = []\n",
        "\n",
        "    for video_path in vid_list:\n",
        "\n",
        "      label        = tf.strings.split(video_path, os.sep)[-2]\n",
        "      temp_image   = []\n",
        "      temp_label   = []\n",
        "    \n",
        "      for image in os.listdir(video_path):\n",
        "        image_path    = video_path + \"/\" + image \n",
        "      \n",
        "        # taking care of labels\n",
        "        temp_label.append(label)\n",
        "\n",
        "        # load the raw data from the file as a string\n",
        "        img = tf.io.read_file(image_path)      \n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, [PX, PY])\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32) # Cast and normalize the image to [0,1]\n",
        "        temp_image.append(img)\n",
        "\n",
        "      image_data.append(temp_image)\n",
        "      label_data.append(temp_label)\n",
        "\n",
        "    return  image_data,  label_data\n",
        "\n",
        "train_data, train_label = build_dataset(video_list)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_data, train_label))\n",
        "\n",
        "## Trying to print the data\n",
        "# for itemx, itemy in dataset.take(3):\n",
        "#   print(itemy.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z03aUWRxT_30",
        "colab": {}
      },
      "source": [
        "def video_path(dataset_dir):\n",
        "  '''\n",
        "  returns the paths of all video files in the dataset; takes input the parent directory\n",
        "  '''\n",
        "  # no. of primitives in the library\n",
        "  prim_actions    = [dI for dI in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir,dI))]\n",
        "\n",
        "  video_path_list = []\n",
        "\n",
        "  for action in prim_actions:\n",
        "    prim_path     = os.path.join(DATA_DIR, action)      # gives path for each primitive  \n",
        "  \n",
        "    for video in os.listdir(prim_path):\n",
        "      video_path  = os.path.join(prim_path, video)      # path to all videos in a prim\n",
        "      video_path_list.append(video_path)\n",
        "      \n",
        "  return video_path_list\n",
        "\n",
        "\n",
        "\n",
        "def get_label(file_path):\n",
        "  ''' \n",
        "  convert the path to a list of path components\n",
        "  '''\n",
        "  label = tf.strings.split(file_path, os.sep)[-2]\n",
        "  \n",
        "  return label\n",
        "\n",
        "# video_file_paths = tf.data.Dataset.from_tensor_slices(video_path(DATA_DIR))\n",
        "# label = get_label(test[0])\n",
        "# print(test[0])\n",
        "# print(label)\n",
        "\n",
        "\n",
        "# this function builds the dataset\n",
        "def build_dataset(vid_path):\n",
        "    print(\"Video Path: \", vid_path)\n",
        "    label        = get_label(vid_path)\n",
        "    temp_image   = []\n",
        "    temp_label   = []\n",
        "    \n",
        "    image_files = tf.data.Dataset.list_files(vid_path + '/*')\n",
        "    print(\"Files: \", image_files)\n",
        "\n",
        "    for image_path in image_files:\n",
        "      print(\"Image Path: \", image_path)\n",
        "      # load the raw data from the file as a string\n",
        "      img = tf.io.read_file(image_path)      \n",
        "      img = tf.image.decode_jpeg(img, channels=3)\n",
        "      img = tf.image.resize(img, [PX, PY])\n",
        "      img = tf.image.convert_image_dtype(img, tf.float32) # Cast and normalize the image to [0,1]\n",
        "      \n",
        "      temp_image.append(img)\n",
        "      temp_label.append(label)\n",
        "\n",
        "    return  tf.data.Dataset.from_tensor_slices(temp_image),  tf.data.Dataset.from_tensor_slices(temp_label)\n",
        "\n",
        "\n",
        "video_file_paths  = tf.data.Dataset.from_tensor_slices(video_path(DATA_DIR))\n",
        "train_dataset_new = video_file_paths.map(build_dataset)\n",
        "\n",
        "# combined_data = tf.data.Dataset.from_tensor_slices(combined)\n",
        "\n",
        "# train_dataset = map(build_dataset, video_path_list, label_list)\n",
        "# train_dataset_new = combined_data.map(build_dataset)\n",
        "# test = map(print_a, string)\n",
        "# print(test)\n",
        "\n",
        "# for point in test.take(2):\n",
        "#   print(point)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCH2tp2CmEbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "6dd34734-ab51-483e-ed0f-864bc23caadf"
      },
      "source": [
        "for file in video_file_paths:\n",
        "  file_n = tf.data.Dataset.list_files(file + '/*')\n",
        "  # file_n = tf.data.Dataset.from_tensor_slices(file_n)\n",
        "  # print(file_n)\n",
        "  print(file)\n",
        "\n",
        "for i in file_n:\n",
        "  print(i)\n",
        "# test = video_path(DATA_DIR)\n",
        "# label = get_label(test[0])\n",
        "# print(test[0])\n",
        "# print(label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Grasp/Video_1', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-59.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-55.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-65.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-62.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-56.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-61.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-68.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-64.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-63.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-54.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-66.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-58.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-51.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-67.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/Approach/Video_1/image-53.jpg', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3JpPA8xve4N",
        "colab_type": "text"
      },
      "source": [
        "### Attempt 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sppfnCGCjI09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# no. of primitives in the library\n",
        "prim_actions    = [dI for dI in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR,dI))]\n",
        "\n",
        "video_path_list = []\n",
        "label_list      = []\n",
        "combined        = []\n",
        "\n",
        "for action in prim_actions:\n",
        "  prim_path  = os.path.join(DATA_DIR, action)   # gives path for each primitive  \n",
        "  \n",
        "  for video in os.listdir(prim_path):\n",
        "    video_path   = os.path.join(prim_path, video)  # path to all videos in a prim\n",
        "\n",
        "    video_path_list.append(video_path)\n",
        "    label_list.append(action)\n",
        "\n",
        "    combined.append([video_path, action])\n",
        "\n",
        "\n",
        "# this function builds the dataset\n",
        "def build_dataset(input):\n",
        "    video_path = input[0].numpy()\n",
        "    # print(video_path)\n",
        "    label      = input[1]\n",
        "    temp_image   = []\n",
        "    temp_label   = []\n",
        "    \n",
        "    for image in os.listdir(video_path):\n",
        "      image_path    = video_path + \"/\" + image \n",
        "      \n",
        "      # taking care of labels\n",
        "      temp_label.append(label)\n",
        "\n",
        "      # load the raw data from the file as a string\n",
        "      img = tf.io.read_file(image_path)      \n",
        "      img = tf.image.decode_jpeg(img, channels=3)\n",
        "      img = tf.image.resize(img, [PX, PY])\n",
        "      img = tf.image.convert_image_dtype(img, tf.float32) # Cast and normalize the image to [0,1]\n",
        "      temp_image.append(img)\n",
        "\n",
        "    return  temp_image,  temp_label\n",
        "\n",
        "# data, labels = build_dataset(video_path, \"Grasp\")\n",
        "# print(data)\n",
        "combined_data = tf.data.Dataset.from_tensor_slices(combined)\n",
        "\n",
        "train_dataset = map(build_dataset, video_path_list, label_list)\n",
        "# train_dataset_new = combined_data.map(build_dataset)\n",
        "# test = map(print_a, string)\n",
        "# print(test)\n",
        "\n",
        "# for point in test.take(2):\n",
        "#   print(point)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F0z71LvSKoS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3d892713-6f87-4c4b-f989-3e712570f6fe"
      },
      "source": [
        "for input_example, target_example in  train_dataset_new.take(1):\n",
        "  print(input_example)\n",
        "  print(target_example)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_VariantDataset shapes: (256, 256, 3), types: tf.float32>\n",
            "<_VariantDataset shapes: (), types: tf.string>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsNo_WO8Gpip",
        "colab_type": "text"
      },
      "source": [
        "### Visualization Dataset [Optional]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC1yLOTZM_sY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c605a0a2-0897-43d5-e498-138f1f6ce037"
      },
      "source": [
        "# images, labels = next(train_ds)\n",
        "\n",
        "# print(np.shape(images))\n",
        "# print(np.shape(labels), labels)\n",
        "# for im in images:\n",
        "#   plt.imshow(im)\n",
        "#   plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30, 400, 400, 3)\n",
            "(30,) [1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0.\n",
            " 0. 1. 0. 0. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8gcZ-1lkpDhK"
      },
      "source": [
        "## Build a model\n",
        "\n",
        "We create a model which take as input movies of shape\n",
        "`(n_frames, width, height, channels)` and returns a movie\n",
        "of identical shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD19WMc5yPHz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "77af16a6-d4a9-4ebf-dafa-143b6d0bbcf7"
      },
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape = (None, PX, PY, CHANNELS)),  \n",
        "        layers.ConvLSTM2D(\n",
        "            filters=64, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ConvLSTM2D(\n",
        "            filters=64, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.ConvLSTM2D(\n",
        "            filters=64, kernel_size=(3, 3), padding=\"same\", return_sequences=True\n",
        "        ),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        # layers.Flatten(),\n",
        "        layers.Dense(100, activation='relu'),\n",
        "        layers.Dense(N_CLASSES, activation='softmax'),\n",
        "        \n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# model compilation\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv_lst_m2d (ConvLSTM2D)    (None, None, 256, 256, 64 154624    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, None, 256, 256, 64 256       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_1 (ConvLSTM2D)  (None, None, 256, 256, 64 295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, None, 256, 256, 64 256       \n",
            "_________________________________________________________________\n",
            "conv_lst_m2d_2 (ConvLSTM2D)  (None, None, 256, 256, 64 295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, None, 256, 256, 64 256       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, None, 256, 256, 64 0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, None, 256, 256, 10 6500      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, None, 256, 256, 2) 202       \n",
            "=================================================================\n",
            "Total params: 752,430\n",
            "Trainable params: 752,046\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hM9IeKmypDhV"
      },
      "source": [
        "## Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-bBV_l6mpDhW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "fe43ecb0-7e06-45a2-c810-a100456fd5d9"
      },
      "source": [
        "epochs = 2\n",
        "\n",
        "model.fit(\n",
        "    image_data,\n",
        "    batch_size=10,\n",
        "    epochs=epochs,\n",
        "    verbose=2,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3c4b0e7c2794>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:531 train_step  **\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:886 __call__\n        self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:158 assert_input_compatibility\n        ' input tensors. Inputs received: ' + str(inputs))\n\n    ValueError: Layer sequential expects 1 inputs, but it received 30 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:8' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:9' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:10' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:11' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:12' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:13' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:14' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:15' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:16' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:17' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:18' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetNext:19' shape=(None, 256, 3) dtype=float32>, <tf.Tensor 'IteratorGetN...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bf_VkgzwpDha"
      },
      "source": [
        "## Test the model on one movie\n",
        "\n",
        "Feed it with the first 7 positions and then\n",
        "predict the new positions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y2qaZa3rpDhb",
        "colab": {}
      },
      "source": [
        " movie_index = 1004\n",
        "track = noisy_movies[movie_index][:7, ::, ::, ::]\n",
        "\n",
        "for j in range(16):\n",
        "    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::])\n",
        "    new = new_pos[::, -1, ::, ::, ::]\n",
        "    track = np.concatenate((track, new), axis=0)\n",
        "\n",
        "\n",
        "# And then compare the predictions\n",
        "# to the ground truth\n",
        "track2 = noisy_movies[movie_index][::, ::, ::, ::]\n",
        "for i in range(15):\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "    ax = fig.add_subplot(121)\n",
        "\n",
        "    if i >= 7:\n",
        "        ax.text(1, 3, \"Predictions !\", fontsize=20, color=\"w\")\n",
        "    else:\n",
        "        ax.text(1, 3, \"Initial trajectory\", fontsize=20)\n",
        "\n",
        "    toplot = track[i, ::, ::, 0]\n",
        "\n",
        "    plt.imshow(toplot)\n",
        "    ax = fig.add_subplot(122)\n",
        "    plt.text(1, 3, \"Ground truth\", fontsize=20)\n",
        "\n",
        "    toplot = track2[i, ::, ::, 0]\n",
        "    if i >= 2:\n",
        "        toplot = shifted_movies[movie_index][i - 1, ::, ::, 0]\n",
        "\n",
        "    plt.imshow(toplot)\n",
        "    plt.savefig(\"%i_animate.png\" % (i + 1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzpOKR_KvZ93",
        "colab_type": "text"
      },
      "source": [
        "### Attempt 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TYSHA-9eoGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8e6bc783-e334-4a5d-8b7c-761081bcef1f"
      },
      "source": [
        "# Root directory of the project\n",
        "datagen  = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "DATA_DIR = \"/content/drive/My Drive/TCS FullTime Work/LfD/Liquid_Pouring/TADL/\"\n",
        "\n",
        "train_ds = datagen.flow_from_directory(    \n",
        "    directory = DATA_DIR, \n",
        "    target_size=(PX, PY),\n",
        "    class_mode='binary', \n",
        "    batch_size=BATCH_SIZE)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 30 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}